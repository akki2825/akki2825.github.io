{"title":"Some intuition on Feature Scaling, Feature Selection and Feature Transformation","markdown":{"yaml":{"layout":"post","title":"Some intuition on Feature Scaling, Feature Selection and Feature Transformation","categories":["machine-learning"],"author":"Akhilesh","date":"2016-12-31","image":"image.png"},"containsRefs":false,"markdown":"\n\nNotes on feature engineering in Machine Learning.\n\n**Feature Scaling**\n\nFeature scaling is a method used to standardize the range of independent variables or features of data. In data processing, it is also known as data normalization and is generally performed during the data pre-processing step.\n\nThe simplest method is rescaling the range of features to scale the range in [0, 1] or [−1, 1]. The general formula is given as:\n\nX’ = (X – Xmin)/(Xmax – Xmin)\n\nwhere, X’ is the value we want to rescale, X is the given value, Xmax is the largest value of X and Xmin the smallest.\n\nLet us consider, old weights = [115,140,175] and we are going to scale for the value 140.\n\nX’ = (140 – 115)/(175 – 115) = 0.41666\n\nTherefore, the range is, [0,0.41666,1]\n\n**Feature Selection**\n\nWhy do we have to perform feature selection?\n\n- Knowledge discovery, Interpretability and to gain some insights.\n- Curse of dimensionality.\n\nThere are two methods of Feature Selection :\n\n- Filtering – Filter type methods select variables regardless of the model. They are based only on general features like the correlation with the variable to predict. Filter methods suppress the least interesting variables. They are mainly used as a pre-process method.\n\n- Set of all features --> Selecting the best subset --> Learning Algorithm --> Performance\n\n- Wrapping – Wrapper methods evaluate subsets of variables which allows, unlike filter approaches, to detect the possible interactions between variables.\n\n**Feature Transformation**\n\nFeature transformation is a group of methods that create new features (predictor variables). Feature selection is a subset of feature transformation.\n\nConsider an ‘X’ space having ‘n’ features, using feature transformation we are going to transform X to have ‘m’ features, where m < n.\n\nThis is done by defining some matrix Px which is a subspace to which we are going to project ‘X’ space. The new features are combination of the old features.\n\nThere are many types explained below –\n\n**Principal Component Analysis (PCA)**\n\nA movie camera takes a 3-D information and flatten it to 2-D without too much loss of information.\n\nWhat does all of this have to do with PCA?\n\nPCA takes a dataset with a lot of dimensions and flatten it to two or three dimensions so we can look at it.\n\nIt tries to find a meaningful way to flatten the data by focusing on the things that are different between cells.\n\nHere, the weights are termed Loadings. And array of loadings is called “Eigen Vector”.\n\nPCA review :\n\n- Systematized way to transform input features into principal components (PC)\n- Use new PCs as new features.\n- PCs are directions in data that maximize variance when you project/compress down onto them.\n- The more variance of data along a PC, the higher that PC is ranked.\n- Most variance, most information would be the first PC.\n- Second-most variance would be the second PC.\n- Max number of PCs = number of input features.\n\nTypical example of PCA is in eigenfaces.\n\nPCA is a linear algebraic approach.\n\n**Independent Components Analysis (ICA)**\n\nIt  is a computational method for separating a multivariate signal into additive sub-components. This is done by assuming that the sub-components are non-Gaussian signals and that they are statistically independent from each other.\n\nA common example application is the “cocktail party problem” of listening in on one person’s speech in a noisy room.\n\nICA is a probabilistic approach.\n\n**Random Component Analysis (RCA)**\n\nUses random way to transform input features into principal components (PC)\n\n**Linear Discriminant Analysis (LDA)**\n\nFinds a projection that descriminates based on the label.\n\n\nFundamental assumption is different, although they do the same thing, which is to capture the original data in some new transform space that is somehow better. \n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"2016-12-31-feature-engineering.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.335","theme":"journal","title-block-banner":true,"layout":"post","title":"Some intuition on Feature Scaling, Feature Selection and Feature Transformation","categories":["machine-learning"],"author":"Akhilesh","date":"2016-12-31","image":"image.png"},"extensions":{"book":{"multiFile":true}}}}}