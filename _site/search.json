[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Akhilesh",
    "section": "",
    "text": "Understanding the spread of Extinct and Endangered languages across India\n\n\n\n\n\n\nnlp\n\n\n\n\n\n\n\n\n\nJul 30, 2020\n\n\nAkhilesh\n\n\n\n\n\n\n\n\n\n\n\n\nSoftwares I use\n\n\n\n\n\n\npersonal-config\n\n\n\n\n\n\n\n\n\nMay 21, 2020\n\n\nAkhilesh\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello! I am Akhilesh Kakolu Ramarao. I am a Natural Language Processing Researcher, currently working as a Wissenschaftlicher Mitarbeiter at the Department of English Language and Linguistics, Heinrich Heine University, Germany.\nI currently work on Computational Morphology – supervised by Prof. Dr. Kevin Tang and Dr. Dinah Baer-Henney.\nI am actively engaged in language revitalization efforts for indigenous communities in Arunachal Pradesh, India, with a focus on Idu Mishmi and K’man Mishmi languages.\nIn my previous life, I have worked on building multilingual chatbots and voice assistants at Skit.ai, as an NLP Researcher at CivicDataLab and as a Software Engineer at Ramco Systems.\nTo know more about my work, you can check my resume here."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/low-resource-langs-india/2020-07-30-endangered-languages-of-india.html",
    "href": "posts/low-resource-langs-india/2020-07-30-endangered-languages-of-india.html",
    "title": "Understanding the spread of Extinct and Endangered languages across India",
    "section": "",
    "text": "Visualizing the spread of low resource languages across India."
  },
  {
    "objectID": "posts/new/comp.html",
    "href": "posts/new/comp.html",
    "title": "Quarto Computations",
    "section": "",
    "text": "import numpy as np\na = np.arange(15).reshape(3, 5)\na\n\narray([[ 0,  1,  2,  3,  4],\n       [ 5,  6,  7,  8,  9],\n       [10, 11, 12, 13, 14]])"
  },
  {
    "objectID": "posts/low-resource-langs-india/2020-07-30-endangered-languages-of-india.html#data-description",
    "href": "posts/low-resource-langs-india/2020-07-30-endangered-languages-of-india.html#data-description",
    "title": "Understanding the spread of Extinct and Endangered languages across India",
    "section": "Data Description",
    "text": "Data Description\nThe dataset of extinct and endangered languages around the world is created by The Guardian and is available here.\nVariables: The name of language, longitude, latitude, degree of endangerment and the number of speakers.\n\nData Visualisation\nLanguages by latitude/longitude and Population are plotted.\nThe interactive visualisation of Languages by Latitude/longitude inform us, where all the extinct and endangered languages are located across India.\nSimilarly, Languages by number of speakers is plotted.\n\n\nCode\n#collapse-hide\nimport numpy as np\nimport pandas as pd\npd.options.mode.chained_assignment = None\n\nfrom IPython.display import HTML\nfrom chart_studio import plotly\nimport plotly.graph_objs as go\nfrom plotly import tools\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode()\n\nlanguage_data = pd.read_csv('data.csv', usecols=[0, 1, 5, 7, 10, 12, 13])\nlanguage_data = language_data.rename(\n    columns={'Name in English':'language', 'Country codes alpha 3':'locations',\n             'Degree of endangerment':'risk', 'Number of speakers':'population'})\nlanguage_data.columns = language_data.columns.str.lower()\nlanguage_data['risk'] = language_data['risk'].str.title()\nlanguage_data['population'] = language_data['population'].fillna(-1)\n\n# endangered or extinct languages in India\nlanguage_ind = language_data[language_data['locations'].str.contains('IND') == True]\n\nlanguage_ind['risk'] = language_ind['risk'].replace(\n    ['Vulnerable', 'Definitely Endangered', 'Severely Endangered',\n     'Critically Endangered', 'Extinct'], [1, 2, 3, 4, 5])\n\nlanguage_ind = language_ind[['language', 'risk', 'population', 'latitude', 'longitude']]\n\n\n\n\nLanguages by Latitude/Longitude\n\n\nCode\n#collapse-hide\nlabels = ['Isolated', 'Threatened', 'Endangered', 'Abandoned', 'Extinct']\ncolors = ['rgb(0, 157, 220)', 'rgb(128, 206, 237)', 'rgb(255, 182, 128)',\n          'rgb(255, 115, 13)', 'rgb(242, 23, 13)']\n\ntraces = []\nfor i in range(1, 6):\n    traces.append(dict(\n        type = 'scattergeo',\n        lon = language_ind[language_ind.risk == i]['longitude'],\n        lat = language_ind[language_ind.risk == i]['latitude'],\n        text = language_ind[language_ind.risk == i]['language'],\n        mode = 'markers',\n        name = labels[i-1],\n        marker = dict( \n            size = 12,\n            opacity = 0.85,\n            color = colors[i-1],\n            line = dict(color = 'rgb(255, 255, 255)', width = 1)\n        )\n    ))\n# print(traces)\nlayout = dict(\n         title = 'Languages by Latitude/Longitude in India (2016)&lt;br&gt;'\n                 '&lt;sub&gt;Click Legend to Display or Hide Categories&lt;/sub&gt;',\n         showlegend = True,\n         legend = dict(\n             x = 0.85, y = 0.4\n         ),\n        geo = dict(\n                 scope = 'asia',\n                 showland = True,\n                 landcolor = 'rgb(250, 250, 250)',\n                 subunitwidth = 1,\n                 subunitcolor = 'rgb(217, 217, 217)',\n                 countrywidth = 1,\n                 countrycolor = 'rgb(217, 217, 217)',\n                 showlakes = True,\n                 lakecolor = 'rgb(255, 255, 255)')\n        )\n\n\nfigure = dict(data = traces, layout = layout)\n\n\n\n\nCode\n#collapse-hide\nimport plotly.io as pio\n\npio.show(figure)\n\n\n                                                \n\n\n\n\nLanguages by Population\n\n\nCode\n#collapse-hide\nlanguage_ind = language_ind.sort_values('population', ascending = False)\nlanguage_ind['text'] = language_ind['language'] + '&lt;br&gt;' + 'Population ' + language_ind[\n                                                                 'population'].astype(str)\n\nnew_traces = []\nfor i in range(1, 6):\n    new_traces.append(dict(\n        type = 'scattergeo',\n        lon = language_ind[language_ind.risk == i]['longitude'],\n        lat = language_ind[language_ind.risk == i]['latitude'],\n        text = language_ind[language_ind.risk == i]['text'],\n        mode = 'markers',\n        name = labels[i-1],\n        hoverinfo = 'text+name',\n        marker = dict( \n            size = (language_ind[language_ind.risk == i]['population'] + 1) ** 0.18 * 6,\n            opacity = 0.85,\n            color = colors[i-1],\n            line = dict(color = 'rgb(255, 255, 255)', width = 1)\n        )\n    ))\n\nnew_layout = dict(\n         title = 'Languages by Population in India (2016)&lt;br&gt;'\n                 '&lt;sub&gt;Click Legend to Display or Hide Categories&lt;/sub&gt;',\n         showlegend = True,\n         legend = dict(\n             x = 0.85, y = 0.4\n         ),\n         geo = dict(\n             scope = 'asia',\n             showland = True,\n             landcolor = 'rgb(250, 250, 250)',\n             subunitwidth = 1,\n             subunitcolor = 'rgb(217, 217, 217)',\n             countrywidth = 1,\n             countrycolor = 'rgb(217, 217, 217)',\n             showlakes = True,\n             lakecolor = 'rgb(255, 255, 255)')\n        )\n\nnew_figure = dict(data = new_traces, layout = new_layout)\n\n\n\n\nCode\n#collapse-hide\nimport plotly.io as pio\n\npio.show(new_figure)"
  },
  {
    "objectID": "posts/linux-filesystem/2016-12-10-linux-file-system.html",
    "href": "posts/linux-filesystem/2016-12-10-linux-file-system.html",
    "title": "Linux Filesystem",
    "section": "",
    "text": "Basics of linux filesystem.\nLinux filesystem dates back to late 1960s, the early days of Unix. Most of the folders are three to four letters long, starting with:\n\n/bin\nBinary programs or executable files which are available to all users.\nIncludes GNU basic command line utilities essential to the system, such as ls, echo, grep etc.\nAlso includes GUI applications\n\n\n/boot\nFiles essential to the booting of the system, such as:\n\nVmlinuz - Linux Kernel\nSystem.map - Memory locations for variables or functions\nInitrd.img - Initial ramdisk, temporary root file system\nGrub\nEFI\n\n\n\n/dev\nFiles for peripheral and component devices of your system:\n\n/dev/js0 - First analogue joystick\n/dev/nvme0 - First NVMe drive\n/dev/sda - First SATA HDD / SSD\n/dev/loop0 - First fake mounting point\n/dev/null - Black Hole\n/dev/random or /dev/urandom - Random number generator\n/dev/zero - Zero\n\n\n\n/etc\nConfiguration files for all programs. However, user specific configs are stored in /home/user/.config\n\n\n/home\nUsers Documents, Pictures, Videos, etc.\nUser specific config settings are stored in /home/user/.config as well as other . (dot) folders.\n\n\n/lib\nShared library files and kernel modules.\nOftern ends with .so (Shared Object) extension.\n\n/lib/modules - Kernel modules\n/lib/x86_64 - Architecture specific\n/lib32 /lib64 - Architecture specific\n\n\n\n/media\nRemovable media, such as CD, DVD, USB Flash drive.\n\n\n/mnt\nFixed drives, such as SATA Hard drive or Solid State Drives.\n\n\n/opt\nAdditional applications which are not part of the default system.\nThis folder is an alternative to /bin, /usr/bin, /usr/local\n\n\n/proc\nPseudo file system which provides an interface to kernel view of processes (e.g. currently running applications). Referred to by PID (Process ID).\n\n\n/root\nRoot users home folder, usually empty if there is no root user on the system. This is not the same as /home/root .\n\n\n/sbin\nBinary programs or executable files which are reserved for access by Root user. Similar to /bin\nIncludes basic command line utilities, such as: blkid, iptables, mount, useradd and visudo.\nAlso includes some GUI applications, although not as many as /bin\n\n\n/srv\nContains data which is served by the system, such as Website or FTP server.\n/var/www is also a location where website data is stored.\n\n\n/sys\nSysfs pseudo-filesystem which provides an interface to kernel view of hardware devices.\nLists more devices than /dev, such as CPU and Firmware.\n\n\n/tmp\nTemporary files which are deleted on shutdown or reboot.\n\n\n/usr\nContains programs and libraries installed by the distro:\n\n/usr/bin - Applications which are available to all users\n/usr/sbin - Applications which are reserved for access by Root user\n/usr/games - Specific folder for games\n/usr/include - C Header files\n/usr/lib - Library files\n/usr/local - Folder for anything locally installed by the system administrator. (Not touched by the distro)\n/usr/share - Manual pages and other documents for applications\n\n\n\n/var\nVariable data files which may regularly change, such as log files and printer spool\n\n/var/backups - Backups of key system files\n/var/cache - Cached data for applications\n/var/crash - Crash data dumps\n/var/lib - Dynamic libraries listed by owner application\n/var/lock - Locked files\n/var/log - Log data\n/var/spool - Spool files for cron, Mail and printing\n/var/tmp - temporary files which need to exist for longer than /tmp allows"
  },
  {
    "objectID": "posts/unsupervised-learning/2016-12-28-unsupervised-learning-clustering.html",
    "href": "posts/unsupervised-learning/2016-12-28-unsupervised-learning-clustering.html",
    "title": "Unsupervised learning: Clustering",
    "section": "",
    "text": "Notes on Unsupervised learning.\nUnsupervised Learning is a type of Machine Learning algorithm used to predict from dataset consisting of input data without labeled responses.\nGetting an algorithm to recognise clumps of data points without any help is called Clustering.\nTypes of Clustering:\n\nMonothetic – Cluster members have some common property.\nPolythetic – Cluster members are similar to each other. There isn’t a single property which makes them all similar.\nHard clustering – clusters do not overlap. That is, each element belongs to a cluster or not.\nSoft Clustering – Clusters may overlap.\nFlat or Hierarchical Clustering.\n\nHere are a few Algorithms used are:\nK-Means Algorithm\nIt works like this: first we choose k, the number of clusters.\n\nAssign – We then find the center of each cluster, they are called Centroids. Choosing the initial location of the centroid affects the final clustering results.\nOptimize – We move the cluster centers to minimize total bands’ length. The web connects the centroids to the data points in that cluster. Thereby, we minimize the total length of the web for each centroid.\n\nLimitations of K-Means\n\nk-means can only be applied when the data points lie in a Euclidean space, failing for more complex types of data.\nEven though we have fixed data points and clusters, we wont get the same result everytime.\nIt is highly dependent on the initial location of the centroid, so, there are chances of us reaching the local minima. More the centroids, more the local minima there is. Hence, we are forced to run the algorithm multiple times.\n\nSingle Linkage Clustering\n\nWe consider each object as a cluster (n objects).\nDefine inter cluster distance between the closest two points in the two clusters.\nMerge two closest clusters.\nRepeat the same, for n-k times, to make k clusters.\nIts just connecting the dots to the nearest ones in a linear fashion.\n\nRunning time is O(n^3) or slightly lesser.\nExpectation Maximization\n\nThe overview of this is similar to K-means, but here, you are going to judge by probability, and you are never assigning one data point fully to one cluster or one cluster wholly to one data point.\nYou instead assign each point partially to each cluster based on the probability that it would belong to the cluster if you fully knew the clusters, and then assign the mean of each cluster based on the assumption that your prior probabilities were correct, and repeat until there are no significant changes.\n\nClustering Properties\n\nRichness – For any assignment of object to clusters, there is some distance from matrix D such that Pd returns that clustering.\nScale-invariance – Scaling distances by a positive value does not change the clustering.\nConsistency – Shrinking intra-cluster distances and expanding inter-cluster distances does not change the clustering.\n\nImpossibility Theorem\nIt states that, no clustering scheme can achieve all three clustering properties.\nYou can find further information, here."
  },
  {
    "objectID": "posts/feature-engineering/2016-12-31-feature-engineering.html",
    "href": "posts/feature-engineering/2016-12-31-feature-engineering.html",
    "title": "Some intuition on Feature Scaling, Feature Selection and Feature Transformation",
    "section": "",
    "text": "Notes on feature engineering in Machine Learning.\nFeature Scaling\nFeature scaling is a method used to standardize the range of independent variables or features of data. In data processing, it is also known as data normalization and is generally performed during the data pre-processing step.\nThe simplest method is rescaling the range of features to scale the range in [0, 1] or [−1, 1]. The general formula is given as:\nX’ = (X – Xmin)/(Xmax – Xmin)\nwhere, X’ is the value we want to rescale, X is the given value, Xmax is the largest value of X and Xmin the smallest.\nLet us consider, old weights = [115,140,175] and we are going to scale for the value 140.\nX’ = (140 – 115)/(175 – 115) = 0.41666\nTherefore, the range is, [0,0.41666,1]\nFeature Selection\nWhy do we have to perform feature selection?\n\nKnowledge discovery, Interpretability and to gain some insights.\nCurse of dimensionality.\n\nThere are two methods of Feature Selection :\n\nFiltering – Filter type methods select variables regardless of the model. They are based only on general features like the correlation with the variable to predict. Filter methods suppress the least interesting variables. They are mainly used as a pre-process method.\nSet of all features –> Selecting the best subset –> Learning Algorithm –> Performance\nWrapping – Wrapper methods evaluate subsets of variables which allows, unlike filter approaches, to detect the possible interactions between variables.\n\nFeature Transformation\nFeature transformation is a group of methods that create new features (predictor variables). Feature selection is a subset of feature transformation.\nConsider an ‘X’ space having ‘n’ features, using feature transformation we are going to transform X to have ‘m’ features, where m < n.\nThis is done by defining some matrix Px which is a subspace to which we are going to project ‘X’ space. The new features are combination of the old features.\nThere are many types explained below –\nPrincipal Component Analysis (PCA)\nA movie camera takes a 3-D information and flatten it to 2-D without too much loss of information.\nWhat does all of this have to do with PCA?\nPCA takes a dataset with a lot of dimensions and flatten it to two or three dimensions so we can look at it.\nIt tries to find a meaningful way to flatten the data by focusing on the things that are different between cells.\nHere, the weights are termed Loadings. And array of loadings is called “Eigen Vector”.\nPCA review :\n\nSystematized way to transform input features into principal components (PC)\nUse new PCs as new features.\nPCs are directions in data that maximize variance when you project/compress down onto them.\nThe more variance of data along a PC, the higher that PC is ranked.\nMost variance, most information would be the first PC.\nSecond-most variance would be the second PC.\nMax number of PCs = number of input features.\n\nTypical example of PCA is in eigenfaces.\nPCA is a linear algebraic approach.\nIndependent Components Analysis (ICA)\nIt is a computational method for separating a multivariate signal into additive sub-components. This is done by assuming that the sub-components are non-Gaussian signals and that they are statistically independent from each other.\nA common example application is the “cocktail party problem” of listening in on one person’s speech in a noisy room.\nICA is a probabilistic approach.\nRandom Component Analysis (RCA)\nUses random way to transform input features into principal components (PC)\nLinear Discriminant Analysis (LDA)\nFinds a projection that descriminates based on the label.\nFundamental assumption is different, although they do the same thing, which is to capture the original data in some new transform space that is somehow better."
  },
  {
    "objectID": "posts/rl/2017-01-13-reinforcement-learning.html",
    "href": "posts/rl/2017-01-13-reinforcement-learning.html",
    "title": "Notes: Reinforcement Learning",
    "section": "",
    "text": "Notes on Reinforcement learning.\nReinforcement learning is the problem faced by an agent that must learn behaviour through trial-and-error interactions with a dynamic environment. It is appropriately thought of as a class of problems, rather than as a set of techniques.\nThere are two main strategies for solving Reinforcement Learning problems. The first is to search in the space for behaviours in order to find one that performs well in the environment. This approach has been taken by work in genetic algorithms and genetic programming. The second is to use statistical techniques and dynamic programming methods to estimate the utility of taking actions in state of the world.\nIn the standard RL model, on each step of interaction the agent receives as input i, some indication of the current state, S, of the environment. The agent chooses an action, a, to generate an output. The action changes the state of the environment and the value of this state transition is communicated to the agent through a scalar reinforcement signal, r. It should choose actions that tend to increase the long-run sum of values of the reinforcement signal. It can learn to do this overtime by systematic trial and error.\nAn intuitive way to understand the relation between the agent and its environment is with the following example:\nEnvironment : You are in state 65. You have 4 possible actions. Agent: I’ll take action 2. Environment: You received a reinforcement of 7 units. You are in state 15. You have 2 possible actions.\nThe agent’s job is to find a policy Π, mapping states to actions, that maximizes some long run measure of reinforcement. We assume the environment is stationary.\nImportant ideas in Reinforcement Learning that came up –\n\nExploration – you have to try unknown actions to get information.\nExploitation – eventually you have to use what you know.\nRegret – even if you learn intelligently you make mistakes.\nSampling – because of chance you have to try things repeatedly.\nDifficulty – learning can be much harder than solving a known MDPs.\n\nMarkov Decision Process\n\nA set of states s∈S\nA set of actions a∈A\nA transition function T(s,a,s’). Also called the model or the dynamics. P(s’|s,a) = T(s,a,s’)\nThe reward function R(s,a,s’)\nStart state\nMaybe a terminal state\nMDP’s are non-deterministic search problems\n\n“Markov” generally means that given the present state, the future and the past are independent. This is just like search, where the successor function could only depend on the current state (not the history). In the deterministic single-agent search problems, we wanted an optimal plan or sequence of actions, from start to goal.\nFor MDPs we want an optimal policy Π* : S ⇒ A\nA policy Π gives an action for each state. An optimal policy is one that maxmizes expected utility if followed. An explicit policy defines a reflex agent.\nDiscounting\nIts reasonable to maximize the sum of rewards and/or to prefer rewards now to rewards later\nOne solution: values of rewards decay exponentially.\nHow to discount? Each time we descend a level. we multiply in the discount once.\nWhy discount? Sooner rewards probably do have higher utility than later rewards. Also helps algorithm converge.\nExample: discount of 0.5 U([1,2,3]) = 11 +20.5 + 3*o.25\nWhat if the game lasts forever? Solution:\n\nFinite horizon: (similar to depth-limited search)\nGives non stationary policies (Π depends on time left)\nDiscounting: use 0<ϒ<1. Smaller ϒ means smaller “horizon”.\n\nAbsorbing state- guarantee that for every policy, a terminal state will eventually be reached.\nHow to solve MDPs?\nThe value (utility) of a state S:\nV*(s) = expected utility starting in S and acting optimally. (tells us how good each state is)\nQ*(s,a) = expected utility starting out having taken action ‘a’ from state ‘s’ and (thereafter) acting optimally. (‘s’ is fixed and we vary ‘a’)\nThe optimal policy: Π(s) = optimal action from state s = argmax Q(s,a) (its the action that achieves the maximum)\nValues of States\nFundamental operation : compute the (expectimax) value of the state.\n\nExpected utility under optimal action.\nAverage sum of (discounted) rewards\nThis is just what expectimax computed!\n\nRecursive definition of value:\nV(s) = max Q(s,a)\nQ*(s,a) = max Σ T(s,a,s’)[R(s,a,s’) + V*(s)] — Bellman’s equation\nQuantities\nPolicy = map of states to actions. Utility = sum of discounted rewards. Values = expected future utility from a state (max node) Q-values = expected future utility from a q-state (chance node)\nBoth value iteration and policy iteration compute the same thing (all optimal values)\nIn Value iteration –\n\nEvery iteration updates both the values and (implicitly) the policy.\nWe dont track the policy, but taking the max over actions implicitly recompute it.\n\nIn policy iteration –\n\nWe do several passes that update utilities with fixed policy (each pass is fast because we consider only one action and not all of them)\nAfter the policy is evaluated, a new policy is chosen (slow like a value iteration pass)\nThe new policy will be better (or we’re done)\n\nUnknown MDP : Model Based Learning\n\nLearn an approximate model based on experiences.\nSolve for values as if the learned model were correct.\nE[A] = ΣP(a).a\n\nUnknown MDP : Model Free Learning\nPassive Reinforcement Learning :\n\nSimplified task: policy evaluation\n\nInput : fixed policy Π(s)\nYou don’t know the transitions T(s,a,s’)\nYou don’t know the rewards R(s,a,s’)\n\nDirect Evaluation: Goal : Compute values for each state under Π Idea : Average together observed sample values. Act according to Π. Everytime you visit a state, write down what the sum of discounted rewards turned out to be. Average those samples.\n\nWhat’s good about direct evaluation?\n\nIts easy to understand.\nIt does’nt require any knowledge of T,R.\nIt eventually computes the correct average values, using just sample transitions.\n\nWhat’s bad about it?\nIt wastes information about state connnection. Each state must be learned seperately. So, it takes log time to learn.\n\nSample based Policy Evaluation: Take samples of outcome s’(by doing action!) and average. We want to improve our estimate of V by computing these averages.\nTemporal Difference Learning: Big Idea: Learn from every experience! Update V(s) each time we experience a transition (s,a,s’,r) Likely outcomes s’ will contribute updates more often.\nTemporal difference learning of values: Policy still fixed, still doing evaluation! Move values toward value of whatever successor occurs : running avg\n\nSample of V(s) : sample = R(s,Π(s),s’) + ϒV(s’)\nUpdate to V(s): V(s) ⇐ (1-α)V(s) + (α)sample Can also be written as : V(s) ⇐ V(s) + (α)[sample – V(s)]\nProblems with TD value Learning: TD value learning is a model – free way to do policy evaluation, mimicking Bellman Updates with running sample averages. Idea : Learn Q-values, not values. Makes action selection model – free too!\nActive Reinforcement Learning\n\nValue iteration:\n\nStart with Q(s,a) = 0, which we know is right.\nGiven Qk, calculate the depth k+1\nQ- values for all Q-states: Qk+1 (s,a) ⇐ ∑ T(s,a,s’)[R(s,a,s’) + ϒmax Qk(s,a)]\n\nQ-learning: Learn Q(s,a) values as you go\n\nReceive a sample (s,a,s’,r)\nConsider your old estimate : Q(s,a)\nConsider your new sample estimate: sample = R(s,a,s’) + ϒmax Q(s’,a’)\nIncorporate the new estimate into a running average: Q(s,a): V(s) ⇐ (1-α)Q(s,a) + (α)[sample] Q-learning converges to optimal policy — even if you’re acting suboptimally! This is called off-policy learning.\n\nCaveats:\n\nYou have to explore enough.\nYou have to eventually mae the learning rate small enough. 3…but not decrease it too quickly.\nBasically, in the limit, it doesn’t matter how you select actions!\n\n\nExploration\nHow to explore?\nSeveral schemes for forcing exploration:\nSimplest: random action (Ε – greedy) - Every time step, flip a coin. - With (small) probability Ε, act normally. - With (large) probability 1-Ε, act on current policy.\nProblems with random actions?\n\nYou do eventually explore the space, but keep thrashing around once learning is done.\nOne solution: lower Ε over time.\nAnother solution : exploration functions\nTake a value estimate u and a, visit count n and returns an optimistic utility e.g : f(u, n) = u + k/n\n\nRegret\n\nEven if you learn the optimal policy, you still make mistakes along the way.\n\nRegret is a measure of your total mistake cost : the difference between your (expected) rewards, including youthful suboptimality, and optimal (expected) rewards.\n\nMimicking regret goes beyond learning t be optimal – it requires optimally learnin g to be optimal.\n\nExample : random exploration and exploration functions both end up optimal, but random exploration has higher regret.\nApproximate Q- learning\nGeneralizing across states:\n\nBasic Q-learning keeps a table of q-values.\nIn realistic situations, we cannot possibly learn about every single state!\nToo many states to visit them all in training.\nToo many states to hold the q-tables in memory.\n\nInstead, we want to generalize:\nLearn about some number of training states from experience. Generalize that experience to new, similar situations. This is a fundamental idea in machine learning.\nSolution: describe a state using a vector of features (properties)\n\nFeatures are functions from state to real numbers (often 0/1) that capture important properties of the state .\nCan also describe a q-state (s,a) with features.\n\nLinear Value Functions\n\nUsing a feature representation, we can write a q-function (or value function ) for any state using a few weights – V(s) = w1f1(s) +w2f2(s)…+wnfn(s) Q(s,a) = w1f1(s,a) + w2f2(s,a)+…+wnfn(s,a)\nAdvantage: Our experience is summed up in a few powerful numbers.\nDisadvantage: States may share features but actually be very different in value !\nQ- learning with linear Q-functions\n\nIntuitive interpretation:\n\nAdjust weights of active features.\n\nExample: if something unexpectedly bad happens, blame the features that were on : disprefer all states with that state’s features.\nPolicy Search\nProblem: often the feature-based policies that work well aren’t the ones that approximate V/Q best.\nSolution: Learn policies that minimize rewards, not the values that predict them.\nPolicy Search: start with an OK solution (eg: Q-learning) then fine-tune by hill climbing on feature weights.\nSimplest policy search :\nStart with an initial linear value function or Q-function. Nudge each feature weight up and down and see if your policy is better than before.\nProblems:\n\nHow do we tell the policy got better?\nNeed to run many sample episodes!\nIf there are a lot of features, that can be impractical.\n\nBetter methods exploit look-ahead structure, sample wisely, change multiple parameters."
  },
  {
    "objectID": "posts/ml-concepts/2017-01-20-ml-concepts.html",
    "href": "posts/ml-concepts/2017-01-20-ml-concepts.html",
    "title": "Notes: Few concepts of machine learning",
    "section": "",
    "text": "Notes on a few ML concepts.\nDeduction: Given the rule and the cause, deduce the effect.\nInduction: Given a cause and an effect, induce a rule.\nAbduction: Given a rule and an effect, abduce a cause.\nTAXONOMY\nWhat? – Parameters, structure, hidden concepts\nWhat from? – Supervised, Unsupervised, Reinforcement\nWhat for? – prediction, diagnostics, summarization\nHow? – passive, active, online, offline\nOutputs? – Classification, Regression\nDetails? – Generative, Discriminative\nOccom’s Razor – Everything else being equal, choose the less complex hypothesis.\nThe Ultimate goal of Machine Learning is to have data models that can learn and improve overtime.\nEvaluation Metrics\nLearn from data to make predictions.\nClassification is about deciding which categories new instances belong to. Then when we see new objects we can use their features to guess which class they belong to.\nIn regression, we want to make a prediction on continuous data.\nIn classification, we want to see how often a model correctly or incorrectly identifies a new example, whereas, in regression we might be more interested to see how far off the model’s prediction is true from true value.\nClassification ⇒ Accuracy, precision, recall and F-score.\nRegression ⇒ mean absolute error and mean square error.\nShort comings of accuracy:\n\nNot ideal for skewed classes\nmay want to err on side of guessing innocent.\nmay want to err on the side of guessing guilty.\n\nCauses of Error:\n\nBias due to a model being unable to represent the complexity of the underlying data.\nVariance due to a model being overly sensitive to the limited data it has been trained on.\nBias occurs when a model has enough data but is not complex enough to capture the underlying relationships. As a result, the model consistently and systematically misrepresents the data, leading to low accuracy in prediction. This is known as Underfitting. To overcome error from bias, we need more complex model.\nVariance is a measure of how much the predictions vary for any given test sample. High sensitivity to the training set is also known as Overfitting. Occurs when the model is too complex.\nWe can typically reduce the variability of a model’s predictions and increase precision by training on more data. If more data is unavailable, we can also control variance by limiting our model’s complexity .\n\nData Types:\n\nNumeric data\nCategorical data\nTime-Series data\n\nCurse of Dimensionality\nAs the number of features or dimensions grows, the amount of data we need to generalize accurately grows exponentially."
  },
  {
    "objectID": "posts/github/2017-01-28-talk-about-github.html",
    "href": "posts/github/2017-01-28-talk-about-github.html",
    "title": "Some talk about Github",
    "section": "",
    "text": "About Github.\nLet’s look at the wonderful world of git, (the Version Control System) and the popular GitHub service that makes creating, developing and sharing code fun and easy.\nGit is open source but GitHub is a for-profit company (and not to be confused with Git). GitHub is by far the largest code repository out there!\nLets take a project folder,( this is what we refer to as repository.) and say we wanted to save it. The save we are going to do is not the normal save, but to take the snapshot of the whole repository as it is. The snapshot here is called commit. Not only can we write comments and share this among people, but we can also go back to that snapshot whenever we wish to.\nImagine we are making these commits the whole time and we get some crazy idea (maybe, add some feature), we can create something called branch. A branch is essentially a new development path that we can follow as long as we like without messing up with our master development plan.\nSay, I have a typo in one of the commits and my buddy figures it out. He’s generous enough to make the necessary changes by making his own branch (and fix the errors himself) . After he’s all done, he can ask me to pull in his changes by using something called Pull Request. If I accept the request, that branch is merged with the master branch.\nGitHub is built to be the place to host the repositories. When we put our repository on GitHub, we then have a centralised place to distribute the code from .\nSay, I have my repo on GitHub,and I pull the code to my local machine and make a few edits, and I’ll commit my edits. Then, I push the code from the local machine to GitHub. Now that my updated code is on GitHub, its easy to invite friends to share the code or distribute something to the world!\nSuppose I have a repo and you are interested in the code but you want to use it for a different purpose. GitHub allows you to fork my repository and make the changes you desire and it creates an entirely new repo for you to manage on your own. Fork is a part of your github account. You can even raise a pull request from your fork. This is how large open source projects are done.\nBesides all this, Github also offers a free static web hosting."
  },
  {
    "objectID": "posts/ml-implementation/2017-01-31-ml-implementation.html",
    "href": "posts/ml-implementation/2017-01-31-ml-implementation.html",
    "title": "ML implementation",
    "section": "",
    "text": "Notes on ML implementation.\nOne secret you need to know beneath all of the math, and algorithms, is the “black art” to actually build machine learning models.\nLearning = Representation + Evaluation + Optimization\nThe first problem faced by people is the bewildering variety of learning algorithms available. Which one to use? There are literally thousands available, and hundreds more published each year. The key to not getting lost in this huge space is to realise that it consists of combinations of just three components. The components are:\nRepresentation : Choosing the representation for a learner is tantamount to choosing the set of classifiers that it can possibly learn. This set is called the hypothesis space.\nEvaluation: An evaluation function is needed to distinguish good classifiers from bad ones. This might be accuracy, f1 score, squared error, information gain etc\nOptimization: Finally, we need a method to search among the classifiers in the language for the high scoring one. The choice of optimization technique is key to the efficiency of the learner, and also helps determine the classifier produced if the evaluation function has more than one optimum.\nThe fundamental goal of machine learning is to generalize beyond the examples in the training set.\nHere are some of the hacks for implementing ML:\nDo not contaminate your classifier by lots of tuning on the test data\nOf course, holding out data reduces the amount available for training. This can be mitigated by doing cross validation: randomly dividing your training data into (say) ten subsets, holding out each one while training on the rest, testing each learned classifier on the examples it did not see, and averaging the results to see how well the particular parameter setting does.\nEvery learner must embody some knowledge or assumptions beyond the data its given in order to generalize beyond it. This was formalized by Wolpart in his famous “no free lunch” theorems, according to which no learner can beat random guessing over all possible function to be learned. Luckily, the function we want to learn in the real world are not drawn uniformly from the set of all mathematically possible functions!\nVery general assumptions – like smoothness, similar examples having similar classes, limited dependencies or limited complexity are often enough to do well, and this is a large part of why ML has been so successful.\nBias is a learner’s tendency to consistently learn the same wrong thing. Variance is the tendency to learn random things irrespective of the real signal.\nBesides cross validation, there are many methods to combat overfitting. Th most popular one is adding a regularization term to the evaluation function. Anther option is to perform a statistical significance test like chi-square before adding new structures to decide whether the distribution of the class really is different with and without this structure. These techniques are particularly useful when data is very scarce.\nGeneralizing correctly becomes exponentially harder as the dimensionality as the dimensionality of the examples grows because a fixed-size training set covers a dwindling fraction of the input space.\n“Blessing of non-uniformity” – in most applications examples are not spread uniformly throughout the instance space, but are concentrated on or near a lower dimensional manifold."
  },
  {
    "objectID": "posts/artificial-neuron/2017-02-08-artificial-neuron.html",
    "href": "posts/artificial-neuron/2017-02-08-artificial-neuron.html",
    "title": "Artificial Neuron",
    "section": "",
    "text": "About artificial neuron.\nArtificial neuron is simply a computational unit, which makes a particular computation based on other units it is connected to.\n\nPre-activation (or input activation): a(x) = b + ∑wi + xi\nNeuron (output) activation: h(x) = g(a(x))\nwhere, g(.) is called activation function, b is called neuron bias and w is the connection weights.\nActivation functions:\n\nLinear activation function\ng(a) = a\nPerforms no input squashing\nSigmoid activation function\ng(a) = sigm(a) = 1/1 + exp(-a)\nSquashes the neuron’s pre-activation between 0 and 1.\nAlways positive\nBounded\nStrictly increasing\nHyperbolic tangent(tanh) activation function\ng(a) = tanh(a)\nSquashes the neuron’s pre-activation between -1 and 1\nCan be positive or negative\nBounded\nStrictly increasing\nRectified Linear Activation function\ng(a) = reclin(a) = max(0,a)\nBounded below by 0 and always non-negative\nNot upper bounded\nMonotomically increasing\nTends to give neurons sparse activities. That is, the value is zero across many different sets of inputs.\n\nCapacity of a single neuron\nCapacity essentially means, the complexity of the computation that a neuron can perform.\n\nIt could do binary classification with Sigmoid, which can interpret neuron as estimating p(y=1|x). That is, the probability of input ‘x’ belonging to class 1. (Since, we are using sigmoid activation function the output is bounded b/w 0 and 1). It is also known as logistic regression classifier. If the value is greater than 0.5, predict class 1, otherwise, predict 0.\nThe classifier is performing linear classification, so, if we have a problem where we want to classify objects described by input vectors into different classes, if we can draw a hyperplane or a line (in case of 2d) between these two types of objects, then a single artificial neuron could do that for us!\nHowever, a single neuron cannot model a non-linearly seperable output.\n\nIf the input is transformed in a better representation, we can actually have a linear seperable.\nThereby, complicated computations cannot be modelled by single neuron. This is the main reason behind why we go for multilayer neural network."
  },
  {
    "objectID": "posts/softwares/2020-05-21-softwares-i-use.html",
    "href": "posts/softwares/2020-05-21-softwares-i-use.html",
    "title": "Softwares I use",
    "section": "",
    "text": "Here are the list of softwares I use for almost everything.\n\n\nArch Linux with all free software.\n\n\n\nLibreWolf with vim key bindings.\n\n\n\nst\n\n\n\nNeovim and Doom Emacs\n\n\n\nNeomutt & Doom Emacs\n\n\n\nmpv\n\n\n\nZathura\n\n\n\ndwm\n\n\n\nlf\n\n\n\nGIMP\n\n\n\nTransmission\n\n\n\nLibreOffice\n\n\n\nlf\n\n\n\nzsh and fish"
  },
  {
    "objectID": "posts/us-foss/2020-05-29-US-government-take-on-FOSS.html",
    "href": "posts/us-foss/2020-05-29-US-government-take-on-FOSS.html",
    "title": "U.S Government’s take on FOSS (Free and Open Source Software)",
    "section": "",
    "text": "Free software is software that respects your freedom.\n\nThis is a running blog on how the U.S government uses and advocates Free and Open Source Software.\nThe US Department of Defence’s (DoD) OSS FAQ states, “continuous and broad peer-review, enabled by publicly available source code, improves software reliability and security through the identification and elimination of defects that might otherwise go unrecognized”.\nFrom the DoD Software Acquisition and Practices (SWAP) study: “DoD should use open source when possible to speed development and deployment and leverage the work of others”.\nDigital Service Playbook was developed by U.S Digital Service and outlines several best practices from the private sector that can help the Government build effective digital services.\nFederal Source Code Policy: “requires agencies, when commissioning new custom software, to release at least 20 percent of new custom-developed code as Open Source Software (OSS)”.\nHere are a few agencies that have embraced the Federal Source Code Policy:\n\nSocial Security Administration (SSA)\nGSA Open Source Software Policy\nNASA Federal Source Code Framework\nEPA: Interim Open Source Software (OSS)\nhttps://code.gov"
  },
  {
    "objectID": "posts/mfa/2021-11-09-mfa.html",
    "href": "posts/mfa/2021-11-09-mfa.html",
    "title": "Installing Kaldi and MFA",
    "section": "",
    "text": "An installation guide.\nThis is a short guide to install Kaldi <https://kaldi-asr.org/> and Montreal Forced Aligner <https://github.com/MontrealCorpusTools/Montreal-Forced-Aligner> for Linux and MacOS users."
  },
  {
    "objectID": "posts/mfa/2021-11-09-mfa.html#kaldi-installation-for-linux-and-macos",
    "href": "posts/mfa/2021-11-09-mfa.html#kaldi-installation-for-linux-and-macos",
    "title": "Installing Kaldi and MFA",
    "section": "Kaldi installation (for Linux and MacOS)",
    "text": "Kaldi installation (for Linux and MacOS)\ngit clone https://github.com/kaldi-asr/kaldi.git kaldi\ncd kaldi/tools\nextras/check_dependencies.sh\nextras/install_mkl.sh\nmake\nextras/install_irstlm.sh\ncd ../src/\n./configure\nmake depend\nmake -j 4\nPro-tip: You will run into errors at some point and all you have to do is to keep calm and read them. They mostly suggest you to install more softwares/tools and that is it."
  },
  {
    "objectID": "posts/mfa/2021-11-09-mfa.html#montreal-forced-aligner-installation",
    "href": "posts/mfa/2021-11-09-mfa.html#montreal-forced-aligner-installation",
    "title": "Installing Kaldi and MFA",
    "section": "Montreal Forced Aligner installation",
    "text": "Montreal Forced Aligner installation\nDownload MFA (version 1.0.1) from here <https://github.com/MontrealCorpusTools/Montreal-Forced-Aligner/releases/tag/v1.0.1)>\nLinux users click on montreal-forced-aligner_linux.tar.gz\nMacOS users click on montreal-forced-aligner_macosx.zip\nJust unzip or untar and follow this detailed guide <https://montreal-forced-aligner.readthedocs.io/en/latest/> for the usage."
  },
  {
    "objectID": "posts/aeneas/2021-12-07-install-aeneas.html",
    "href": "posts/aeneas/2021-12-07-install-aeneas.html",
    "title": "Installing Aeneas on WSL and MacOS",
    "section": "",
    "text": "An installation guide.\nThis is a detailed guide to install Aeneas (https://github.com/akki2825/aeneas) that supports python3.6+ on Windows Subsystem for Linux (WSL) and MacOS."
  },
  {
    "objectID": "posts/aeneas/2021-12-07-install-aeneas.html#windows-subsystem-for-linux-wsl",
    "href": "posts/aeneas/2021-12-07-install-aeneas.html#windows-subsystem-for-linux-wsl",
    "title": "Installing Aeneas on WSL and MacOS",
    "section": "Windows Subsystem for Linux (WSL)",
    "text": "Windows Subsystem for Linux (WSL)\n\nGo to Search bar on the bottom left of the screen (next to the Windows icon) and search for Command Prompt. Right click on Command Prompt icon and select Run as administrator. This should open the command prompt.\nType wsl --install and press Enter. If you run into any errors, it is most likely the case that your windows version doesn’t support WSL or you’re internet is unstable.\nThe above process might take a few minutes. Once done, go to your browser and type https://aka.ms/wslstore and press Enter.\nOnce the above webpage loads, scroll a bit down and click on Ubuntu. Subsequently click on Get and Install.\nOnce the Ubuntu application opens, enter a username and password that you can frequently remember.\n\nCopy and Paste (right-click on the terminal to paste) the below code:\nsudo apt-get update\nsudo apt-get upgrade\n\nEasy Installation\nDownload the script from here (https://gist.github.com/akki2825/3b38a9f33170b31617b141e53745565b) and run this shell script using:\ngit clone https://gist.github.com/akki2825/3b38a9f33170b31617b141e53745565b\ncd 3b38a9f33170b31617b141e53745565b/\nsh install_py3_aeneas.sh\n\n\nDetailed Installation\nClone the py3-aeneas repository\ngit clone https://github.com/akki2825/aeneas\ncd aeneas/\nInstall system dependencies\nsudo apt-get install -y python3-dev\nsudo apt-get install python3-pip\nsudo apt-get install make autoconf automake libtool pkg-config\nsudo apt-get install libespeak-dev\nsudo apt-get install ffmpeg\nsudo apt-get install espeak\nsudo apt-get install espeak-data\nInstall Python packages\nsudo pip3 install numpy\nsudo pip3 install py3-aeneas\nCompile Python C/C++ extensions\nsudo python3 setup.py build_ext --inplace\nCheck Setup\nsudo python3 -m aeneas.diagnostics"
  },
  {
    "objectID": "posts/aeneas/2021-12-07-install-aeneas.html#macos",
    "href": "posts/aeneas/2021-12-07-install-aeneas.html#macos",
    "title": "Installing Aeneas on WSL and MacOS",
    "section": "MacOS",
    "text": "MacOS\nCopy and Paste the below code in the terminal (using Command+C and Command+V):\nInstall Brew\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n\nEasy Installation\nDownload the script from here (https://gist.github.com/akki2825/8014a2800eaf638eee2989974655c98f) and run this shell script using:\ngit clone https://gist.github.com/akki2825/8014a2800eaf638eee2989974655c98f\ncd 8014a2800eaf638eee2989974655c98f/\nsh install_py3_aeneas.sh\n\n\nDetailed installation\nClone the py3-aeneas repository\ngit clone https://github.com/akki2825/aeneas\ncd aeneas/\nInstall system dependencies\nbrew install python3\nbrew install python3-dev\nbrew install python3-pip\nbrew install ffmpeg\nbrew install espeak\nbrew install espeak-data\nInstall Python packages\nsudo pip3 install numpy\nsudo pip3 install py3-aeneas\nCompile Python C/C++ extensions\nsudo python3 setup.py build_ext --inplace\nCheck Setup\nsudo python3 -m aeneas.diagnostics"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About",
    "section": "Experience",
    "text": "Experience\n\nEmployment\n\nHeinrich-Heine-Universität Düsseldorf\n\nPosition: Wissenschaftlicher Mitarbeiter, Department of English Language and Linguistics\n\nWorking on Computational Morphology. Supervised by Prof. Dr. Kevin Tang and mentored by Dr. Dinah Baer-Henney.\nTeaching Computational Linguistics courses for BA and MA students.\nIT administration for Slamlab.\n\n\n\n\nCivicDataLab\n\nIntegral role in developing and maintaining partnerships with various NGOs and Governments.\nWorked on building Language Platform for Assamese language, including Optical Character Recognition, Machine Translation system for English and Assamese, Assamese keyboard, Rich text editor, and creation of Parallel Corpora.\nCase studies include analyzing language complexity of Union Budget Speeches, topic modeling on Union Budget suggestions, and building Crowd-sourced Translation platform for COVID-19 information dissemination.\nInvolved in building Text Annotation platform for annotating entities and relations in Indian Court judgements.\n\n\n\nVernacular.ai\n\nEarly stage involvement allowed for building technical components in Voice Assistants and Multi-lingual Chatbots.\nDesigned and developed end-to-end speech and natural language processing A.I. systems for automating Customer Care Systems in the hotel industry.\n\n\n\nRamco Systems India\n\nPosition: Software Engineer, Bengaluru/Chennai 2017\n\nPrime developer in designing and developing a Chatbot for Human Resource (HR) system.\nResponsible for bug fixes, documentation, and feature implementation of Machine Learning components.\nCrafted users’ conversations using Finite State Machines.\n\n\n\n\n\nLanguage Revitalization\n\nOrganization: Idu Mishmi Cultural and Literary Society (IMCLS), India\nRole: Chief Coordinator, Arunachal Pradesh 2020\n\nInstrumental in initiating the Language Revitalization Movement.\nBuilt the first ever E-Dictionary for Idu Mishmi and English.\n\n\n\n\nConsulting\n\nXRI Institute, USA: Developed tools for Human Language Technology (HLT) for low-resource languages.\nVrook, India: Tech Consultant focusing on online pedagogy and learning platforms.\n\n\n\nInternships\n\nSense Infinity Technologies, India: Full Stack Developer Intern 2016.\nMaruti Suzuki, India: Industry Trainee 2014.\n\n\n\nVolunteering\n\nLohit Youth Library Network, India: Volunteer, Arunachal Pradesh 2020 - Present.\n\n\n\nFreelancing\n\nDesigned and developed a search tool for Online (Business) Listings using Python and Elasticsearch in 2017."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\n\nM.A., Linguistics\nHeinrich-Heine-Universität Düsseldorf, Germany, 2022-2024\n\n\nB.E., Automobile Engineering\nDayananda Sagar College of Engineering, Bengaluru, India, 2012-2016"
  },
  {
    "objectID": "about.html#teaching",
    "href": "about.html#teaching",
    "title": "About",
    "section": "Teaching",
    "text": "Teaching\n\nLLMs in Linguistic Research, Winter, 2024\nLearning Inflection, Summer, 2024\nAccent unplugged: Evaluating Voice assistants, Winter, 2023\nSynthesizing speech, Summer, 2023\nLanguage technology for Linguists with Internet of Things (IoT), Winter, 2022\nLaTeX for Linguists, Winter, 2022\nTowards a career in language technology: linguistic annotation, Summer, 2022\nBasic Skills in Digital Humanities, Winter, 2021"
  },
  {
    "objectID": "about.html#skills-and-projects",
    "href": "about.html#skills-and-projects",
    "title": "About",
    "section": "Skills and Projects",
    "text": "Skills and Projects\n\nTechnical Skills\n\nRecent: Python, Bash, LaTeX, Scheme\nPast: Haskell, Dart, JavaScript (AngularJS), Frameworks: Keras, Tensorflow, Flutter\nPlatforms: Arch Linux, Ubuntu\n\n\n\nPersonal Projects\n\nContributions to open source and analysis of Corrosion and Wear behavior in Material Science."
  },
  {
    "objectID": "about.html#grantsawards",
    "href": "about.html#grantsawards",
    "title": "About",
    "section": "Grants/Awards",
    "text": "Grants/Awards\n\nFirst prize at the HHU Legal Hackathon 2022. Team: Akhilesh Kakolu Ramarao, Phaedon Paschalis and Diana Schill.\nFirst prize at the HeiCAD Lightning Talks 2022.\nHardware Grant. 20 × NVIDIA® JetsonTM Nano Developer Kits. NVIDIA Academic Hardware Grant, “FOSStering Digital Humanities through Accent Diversity & Conversational Devices”. Co-applicant: Kevin Tang\nEducation Grant. €9,424. e-Learning support fund, Heinrich-HeineUniversity Düsseldorf. “Interactive web-based review units for phonetics and phonology”. Co-applicant: Christopher Geissler and Kevin Tang"
  },
  {
    "objectID": "about.html#publications",
    "href": "about.html#publications",
    "title": "About",
    "section": "Publications",
    "text": "Publications\n\nKakolu Ramarao, A., Petersen, W., Stein, A., Stein, E., & Xia, H. (2024). KlarTextCoders at StaGE: Automatic Statement Annotations for German Easy Language. In Proceedings of GermEval 2024 Shared Task on Statement Segmentation in German Easy Language (StaGE) (pp. 15–27), Vienna, Austria. Association for Computational Lingustics. https://aclanthology.org/2024.germeval-1.2.pdf\nMuschalik, J., Schmitz, D., Kakolu Ramarao, A. et al. Typing /s/-morphology between the keys?. Read Writ (2024). DOI\nJeong, C., Schmitz, D., Kakolu Ramarao, A., Stein, A., & Tang, K. (2023). Linear discriminative learning: a competitive non-neural baseline for morphological inflection. In Proceedings of the 20th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology (pp. 138-150). Association for Computational Linguistics. https:// aclanthology.org/2023.sigmorphon-1.16.pdf\nKakolu Ramarao, A., Zinova, Y., Tang, K., & van de Vijver, R. (2022). HeiMorph at SIGMORPHON 2022 shared task on morphological acquisition trajectories. In Proceedings of the 19th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology (pp. 236-239). Association for Computational Linguistics. https:// aclanthology.org/2022.sigmorphon-1.24"
  },
  {
    "objectID": "about.html#presentations",
    "href": "about.html#presentations",
    "title": "About",
    "section": "Presentations",
    "text": "Presentations\n\n“From Zero to Terminal Hero” at 32nd TaCoS conference, 2023\n“Can Neural Networks learn human-like behavior when inflecting verbs? The case of Spanish” at the Five-Minute Linguist competition, Linguistic Society of America 97th Annual Meeting, 2023\n“Modeling irregular morphological patterns using Transformers” at HeiCAD Lightning Talks, 2022\n“Language Revitalization: A case for Idu Mishmi” at Microsoft Research India and Navana Tech, 2021"
  },
  {
    "objectID": "about.html#conferenceworkshop-organizer",
    "href": "about.html#conferenceworkshop-organizer",
    "title": "About",
    "section": "Conference/Workshop Organizer",
    "text": "Conference/Workshop Organizer\n\nChief organizer of LibreSlam at Heinrich-Heine-University, Germany, 2022.\nChief organizer and teaching member of the LY-CS101 Course in collaboration with National College, Bangalore, India, and Merge Intern, 2020."
  },
  {
    "objectID": "about.html#research-interests",
    "href": "about.html#research-interests",
    "title": "About",
    "section": "Research Interests",
    "text": "Research Interests\n\nComputational Morphology\nLanguage Revitalization"
  },
  {
    "objectID": "about.html#academic-positions",
    "href": "about.html#academic-positions",
    "title": "About",
    "section": "Academic Positions",
    "text": "Academic Positions\n\nHeinrich-Heine-Universität Düsseldorf, Germany\n2021-present\nPosition: Wissenschaftlicher Mitarbeiter, Department of English Language and Linguistics\n\nTeaching computational linguistics courses for BA and MA students. Course list\nResponsible for IT administration for Slamlab"
  },
  {
    "objectID": "about.html#research-experience",
    "href": "about.html#research-experience",
    "title": "About",
    "section": "Research Experience",
    "text": "Research Experience\n\nCivicDataLab\n\nDeveloped Language Platform for Assamese, including Optical Character Recognition, Machine Translation system, keyboard, rich text editor, and parallel corpora creation.\nConducted case studies on language complexity of Union Budget Speeches and topic modeling on Union Budget suggestions.\nBuilt crowd-sourced translation platform for COVID-19 information dissemination in local languages.\nInvolved in developing a text annotation platform for sequence labeling of entities and relations in Indian court judgments.\n\n\n\nVernacular.ai\n\nDesigned and developed end-to-end speech and natural language processing AI systems for automating customer care in the hotel industry.\nImplemented conversation design framework, dialog management system, and user response prediction strategies.\n\n\n\nRamco Systems India\n\nDeveloped a chatbot for Human Resource (HR) system, responsible for bug fixes, documentation, and feature implementation of machine learning components.\nCrafted user conversations using Finite State Machines."
  },
  {
    "objectID": "about.html#language-revitalization",
    "href": "about.html#language-revitalization",
    "title": "About",
    "section": "Language Revitalization",
    "text": "Language Revitalization\n\nIdu Mishmi Cultural and Literary Society (IMCLS), India\n2020-present\nRole: Chief Coordinator\n\nManaging partnerships and technology\nDeveloped an android E-Dictionary application for the Idu Mishmi language"
  },
  {
    "objectID": "about.html#poster-presentations",
    "href": "about.html#poster-presentations",
    "title": "About",
    "section": "Poster Presentations",
    "text": "Poster Presentations\n\nPeer-reviewed Posters\n\nRamarao, A. K., & Stein, A. S. AnglistikVoices: an L2 English speech dataset for educational and technological advancement in speech technology. LabPhon 19, Hangyang University, Seoul, South Korea, 2024. PDF\nMuschalik, J., Schmitz, D., Kakolu Ramarao, A., & Baer-Henney, D. (2023). Acoustic duration and typing timing - same, same… but different? Phonetics and Phonology in Europe 2023, Nijmegen, Netherlands.\nTang, K., Kakolu Ramarao, A., & Baer-Henney, D. (2022). Modeling irregular morphological patterns with recurrent neural network: the case of the L-shaped morphome. 13th Mediterranean Morphology Meeting, University of the Aegean, Greece.\n\n\n\nInvited Posters\n\nKakolu Ramarao, A. (2020). Analysis of POCSO cases in Uttar Pradesh. Agami, India, Bengaluru."
  },
  {
    "objectID": "about.html#conferenceworkshop-organization",
    "href": "about.html#conferenceworkshop-organization",
    "title": "About",
    "section": "Conference/Workshop Organization",
    "text": "Conference/Workshop Organization\n\nTranslation Workshop: English to K’man Mishmi, 2023, Bamboosa Library, Tezu, Arunachal Pradesh, India\nChief Organizer, LibreSlam - Free and Open Source (FOSS) for everybody, Heinrich-Heine-University, Germany, 2022\nChief Organizer and Teaching Member, LY-CS101 2020, National College, Bengaluru, India."
  },
  {
    "objectID": "about.html#industry-experience",
    "href": "about.html#industry-experience",
    "title": "About",
    "section": "Industry Experience",
    "text": "Industry Experience\n\nCivicDataLab\n2019-2020\nPosition: NLP Researcher\n\nDeveloped Language Platform for Assamese, including Optical Character Recognition, Machine Translation system, keyboard, rich text editor, and parallel corpora creation\nConducted case studies on language complexity of Union Budget Speeches and topic modeling on Union Budget suggestions\nBuilt crowd-sourced translation platform for COVID-19 information dissemination in local languages\nInvolved in developing a text annotation platform for sequence labeling of entities and relations in Indian court judgments\n\n\n\nSkit.ai (previously, Vernacular.ai)\n2017-2019\nPosition: Software Engineer\n\nInvolved in building multilingual chatbots and voice assistants.\nImplemented conversation design framework, dialog management system, and user response prediction strategies\n\n\n\nRamco Systems\n2016-2017\nPosition: Software Engineer\n\nDeveloped a chatbot for Human Resource (HR) system, responsible for bug fixes, documentation, and feature implementation of machine learning components"
  },
  {
    "objectID": "about.html#vernacular.ai",
    "href": "about.html#vernacular.ai",
    "title": "About",
    "section": "### Vernacular.ai",
    "text": "### Vernacular.ai\n\nImplemented conversation design framework, dialog management system, and user response prediction strategies.\n\n\nRamco Systems India\n\nDeveloped a chatbot for Human Resource (HR) system, responsible for bug fixes, documentation, and feature implementation of machine learning components.\nCrafted user conversations using Finite State Machines."
  },
  {
    "objectID": "about.html#skills",
    "href": "about.html#skills",
    "title": "About",
    "section": "Skills",
    "text": "Skills\n\nRecent experience: Python, Bash, LaTeX, Scheme, Julia\nPast experience: Haskell, Dart, JavaScript (AngularJS)\nFrameworks: PyTorch, Flutter\nPlatforms: Arch linux, Ubuntu\nCloud: AWS, GCP, High-performance computing (HPC)"
  },
  {
    "objectID": "about.html#talks",
    "href": "about.html#talks",
    "title": "About",
    "section": "Talks",
    "text": "Talks\n\nPeer-reviewed Talks\n\n“Community-driven Language Revitalization: Translating Children’s Literature in K’man Mishmi” at International Conference on Language, Ecology and Culture 2024, NEILAC, Guwahati, India. Team: Akhilesh Kakolu Ramarao, Satyanarayan Mundayoor, Pamir Gogoi, Sakelu Chikro. Presenter: Sakelu Chikro. PDF\n“KlarTextCoders at StaGE: Automatic Statement Annotations for German Easy Language” at KONVENS 2024, University of Vienna, Austria. Team: Akhilesh Kakolu Ramarao, Wiebke Petersen, Anna Sophia Stein, Emma Stein, Hanxin Xia. Presenter: Akhilesh Kakolu Ramarao and Emma Stein. PDF\n“A computational approach to understanding the cognitive reality of morphomic patterns: the case of L-shaped morphomes in Spanish” at Annual Meeting of Linguistics Association of Great Britain and Northern Ireland 2024, Newcastle University, United Kingdom. Team: Akhilesh Kakolu Ramarao, Kevin Tang and Dinah Baer-Henney. Presenter: Akhilesh Kakolu Ramarao PDF\n“Acoustic Analysis of Vowels in Idu Mishmi” at the 27th Himalayan Languages Symposium, 2024, IIT Guwahati, India. Abstract. Presenter: Pamir Gogoi. Team: Pamir Gogoi, Akhilesh Kakolu Ramarao and Ria Borah Sonowal.\n“From Zero to Terminal Hero” at the 32nd Tagung der Computerlinguistik-Studierenden (TaCoS), 1 July 2023, Heinrich Heine University, Düsseldorf, Germany. PDF\n“Can Neural Networks learn human-like behavior when inflecting verbs? The case of Spanish” at the Five-Minute Linguist competition, Linguistic Society of America 97th Annual Meeting, 2023\n“Modeling irregular morphological patterns using Transformers” at HeiCAD Lightning Talks, 2022\n\n\n\nInvited Talks\n\nLanguage Revitalization: A case of Idu Mishmi at Microsoft Research India, 2021\nLanguage Revitalization at Navana Tech, 2021"
  },
  {
    "objectID": "about.html#volunteering",
    "href": "about.html#volunteering",
    "title": "About",
    "section": "Volunteering",
    "text": "Volunteering\n\nIdu Mishmi Cultural and Literary Society (IMCLS)\n2020-present\nRole: Chief Coordinator\n\nManaging partnerships and technology\nDeveloped an android E-Dictionary application for the Idu Mishmi language\n\n\n\nLohit Youth Library Network\n2020-present\nRole: Volunteer"
  },
  {
    "objectID": "about.html#consulting",
    "href": "about.html#consulting",
    "title": "About",
    "section": "Consulting",
    "text": "Consulting\n\nXRI Institute\n2020-2021\n\n\nVrook\n2020-2021"
  },
  {
    "objectID": "about.html#internships",
    "href": "about.html#internships",
    "title": "About",
    "section": "Internships",
    "text": "Internships\n\nSense Infinity Technologies\nPosition: Full-stack developer intern\n\n\nMaruti Suzuki\nPosition: Industry trainee"
  },
  {
    "objectID": "about.html#demos",
    "href": "about.html#demos",
    "title": "About",
    "section": "Demos",
    "text": "Demos\n\nStudents as Researchers. In Nacht der Wissenschaft. Shadowplatz, Düsseldorf, Germany, 2024.\nRole of linguistics in AI for social good. In Nacht der Wissenschaft. Shadowplatz, Düsseldorf, Germany, 2022."
  }
]